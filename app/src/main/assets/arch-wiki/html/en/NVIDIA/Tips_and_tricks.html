<!DOCTYPE html>
<html lang="en" dir="ltr" class="client-nojs">
<head>
<meta charset="UTF-8">
<title>NVIDIA/Tips and tricks - ArchWiki</title>
<link rel="stylesheet" href="../../ArchWikiOffline.css">
<meta name="ResourceLoaderDynamicStyles" content="">
<style></style>
<meta name="generator" content="MediaWiki 1.26.4">
<meta name="robots" content="noindex,follow">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/opensearch_desc.php" title="ArchWiki (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://wiki.archlinux.org/api.php?action=rsd">
<link rel="copyright" href="http://www.gnu.org/copyleft/fdl.html">
<link rel="alternate" type="application/atom+xml" title="ArchWiki Atom feed" href="/index.php?title=Special:RecentChanges&amp;feed=atom">
</head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-NVIDIA_Tips_and_tricks skin-archlinux action-view">

		<div id="globalWrapper" style="width: 100%">
		<div id="column-content">
			<div id="content" class="mw-body" role="main" style="margin: 2em; margin-bottom: 0">
				<a id="top"></a>
				
				<div class="mw-indicators">
</div>
				<h1 id="firstHeading" class="firstHeading" lang="en">NVIDIA/Tips and tricks</h1>
				
				<div id="bodyContent" class="mw-body-content">
					<div id="contentSub"><span class="subpages">&lt; <a href="../../en/NVIDIA.html" title="NVIDIA">NVIDIA</a></span></div>
										<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr">
<div id="toc" class="toc">
<div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Fixing_terminal_resolution"><span class="tocnumber">1</span> <span class="toctext">Fixing terminal resolution</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Using_TV-out"><span class="tocnumber">2</span> <span class="toctext">Using TV-out</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#X_with_a_TV_.28DFP.29_as_the_only_display"><span class="tocnumber">3</span> <span class="toctext">X with a TV (DFP) as the only display</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Check_the_power_source"><span class="tocnumber">4</span> <span class="toctext">Check the power source</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Listening_to_ACPI_events"><span class="tocnumber">5</span> <span class="toctext">Listening to ACPI events</span></a></li>
<li class="toclevel-1 tocsection-6">
<a href="#Displaying_GPU_temperature_in_the_shell"><span class="tocnumber">6</span> <span class="toctext">Displaying GPU temperature in the shell</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#nvidia-settings"><span class="tocnumber">6.1</span> <span class="toctext">nvidia-settings</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#nvidia-smi"><span class="tocnumber">6.2</span> <span class="toctext">nvidia-smi</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#nvclock"><span class="tocnumber">6.3</span> <span class="toctext">nvclock</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Set_fan_speed_at_login"><span class="tocnumber">7</span> <span class="toctext">Set fan speed at login</span></a></li>
<li class="toclevel-1 tocsection-11">
<a href="#Manual_configuration"><span class="tocnumber">8</span> <span class="toctext">Manual configuration</span></a>
<ul>
<li class="toclevel-2 tocsection-12"><a href="#Disabling_the_logo_on_startup"><span class="tocnumber">8.1</span> <span class="toctext">Disabling the logo on startup</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Overriding_monitor_detection"><span class="tocnumber">8.2</span> <span class="toctext">Overriding monitor detection</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Enabling_brightness_control"><span class="tocnumber">8.3</span> <span class="toctext">Enabling brightness control</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Enabling_SLI"><span class="tocnumber">8.4</span> <span class="toctext">Enabling SLI</span></a></li>
<li class="toclevel-2 tocsection-16">
<a href="#Enabling_overclocking"><span class="tocnumber">8.5</span> <span class="toctext">Enabling overclocking</span></a>
<ul>
<li class="toclevel-3 tocsection-17"><a href="#Setting_static_2D.2F3D_clocks"><span class="tocnumber">8.5.1</span> <span class="toctext">Setting static 2D/3D clocks</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

<h2><span class="mw-headline" id="Fixing_terminal_resolution">Fixing terminal resolution</span></h2>
<p>Transitioning from nouveau may cause your startup terminal to display at a lower resolution. For GRUB, see <a href="../../en/GRUB/Tips_and_tricks.html#Setting_the_framebuffer_resolution" title="GRUB/Tips and tricks">GRUB/Tips and tricks#Setting the framebuffer resolution</a> for details.
</p>
<h2><span class="mw-headline" id="Using_TV-out">Using TV-out</span></h2>
<p>A good article on the subject can be found <a rel="nofollow" class="external text" href="http://en.wikibooks.org/wiki/NVidia/TV-OUT">here</a>.
</p>
<h2><span class="mw-headline" id="X_with_a_TV_.28DFP.29_as_the_only_display">X with a TV (DFP) as the only display</span></h2>
<p>The X server falls back to CRT-0 if no monitor is automatically detected. This can be a problem when using a DVI connected TV as the main display, and X is started while the TV is turned off or otherwise disconnected.
</p>
<p>To force NVIDIA to use DFP, store a copy of the EDID somewhere in the filesystem so that X can parse the file instead of reading EDID from the TV/DFP.
</p>
<p>To acquire the EDID, start nvidia-settings. It will show some information in tree format, ignore the rest of the settings for now and select the GPU (the corresponding entry should be titled "GPU-0" or similar), click the <code>DFP</code> section (again, <code>DFP-0</code> or similar), click on the <code>Acquire Edid</code> Button and store it somewhere, for example, <code>/etc/X11/dfp0.edid</code>.
</p>
<p>If in the front-end mouse and keyboard are not attached, the EDID can be acquired using only the command line. Run an X server with enough verbosity to print out the EDID block:
</p>
<pre>$ startx -- -logverbose 6
</pre>
<p>After the X Server has finished initializing, close it and your log file will probably be in <code>/var/log/Xorg.0.log</code>. Extract the EDID block using nvidia-xconfig:
</p>
<pre>$ nvidia-xconfig --extract-edids-from-file=/var/log/Xorg.0.log --extract-edids-output-file=/etc/X11/dfp0.bin
</pre>
<p>Edit <code>xorg.conf</code> by adding to the <code>Device</code> section:
</p>
<pre>Option "ConnectedMonitor" "DFP"
Option "CustomEDID" "DFP-0:/etc/X11/dfp0.edid"
</pre>
<p>The <code>ConnectedMonitor</code> option forces the driver to recognize the DFP as if it were connected. The <code>CustomEDID</code> provides EDID data for the device, meaning that it will start up just as if the TV/DFP was connected during X the process.
</p>
<p>This way, one can automatically start a display manager at boot time and still have a working and properly configured X screen by the time the TV gets powered on.
</p>
<p>If the above changes did not work, in the <code>xorg.conf</code> under <code>Device</code> section you can try to remove the <code>Option "ConnectedMonitor" "DFP"</code> and add the following lines:
</p>
<pre>Option "ModeValidation" "NoDFPNativeResolutionCheck"
Option "ConnectedMonitor" "DFP-0"
</pre>
<p>The <code>NoDFPNativeResolutionCheck</code> prevents NVIDIA driver from disabling all the modes that do not fit in the native resolution.
</p>
<h2><span class="mw-headline" id="Check_the_power_source">Check the power source</span></h2>
<p>The NVIDIA X.org driver can also be used to detect the GPU's current source of power. To see the current power source, check the 'GPUPowerSource' read-only parameter (0 - AC, 1 - battery):
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-settings -q GPUPowerSource -t</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">1</pre>
<h2><span class="mw-headline" id="Listening_to_ACPI_events">Listening to ACPI events</span></h2>
<p>NVIDIA drivers automatically try to connect to the <a href="../../en/Acpid.html" title="Acpid">acpid</a> daemon and listen to ACPI events such as battery power, docking, some hotkeys, etc. If connection fails, X.org will output the following warning:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">~/.local/share/xorg/Xorg.0.log</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">
NVIDIA(0): ACPI: failed to connect to the ACPI event daemon; the daemon
NVIDIA(0):     may not be running or the "AcpidSocketPath" X
NVIDIA(0):     configuration option may not be set correctly.  When the
NVIDIA(0):     ACPI event daemon is available, the NVIDIA X driver will
NVIDIA(0):     try to use it to receive ACPI event notifications.  For
NVIDIA(0):     details, please see the "ConnectToAcpid" and
NVIDIA(0):     "AcpidSocketPath" X configuration options in Appendix B: X
NVIDIA(0):     Config Options in the README.
</pre>
<p>While completely harmless, you may get rid of this message by disabling the <code>ConnectToAcpid</code> option in your <code>/etc/X11/xorg.conf.d/20-nvidia.conf</code>:
</p>
<pre>Section "Device"
  ...
  Driver "nvidia"
  Option "ConnectToAcpid" "0"
  ...
EndSection
</pre>
<p>If you are on laptop, it might be a good idea to install and enable the <a href="../../en/Acpid.html" title="Acpid">acpid</a> daemon instead.
</p>
<h2><span class="mw-headline" id="Displaying_GPU_temperature_in_the_shell">Displaying GPU temperature in the shell</span></h2>
<h3><span class="mw-headline" id="nvidia-settings">nvidia-settings</span></h3>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDDDFF; border: thin solid #BBBBDD; overflow: hidden;">
<strong> Note: </strong>This method requires that you are using X. Use Method 2 or Method 3 if you are not. Also note that Method 3 currently does not not work with newer NVIDIA cards such as GeForce 200 series cards as well as embedded GPUs such as the Zotac IONITX's 8800GS.</div>
<p>To display the GPU temp in the shell, use <code>nvidia-settings</code> as follows:
</p>
<pre>$ nvidia-settings -q gpucoretemp
</pre>
<p>This will output something similar to the following:
</p>
<pre>Attribute 'GPUCoreTemp' (hostname:0.0): 41.
'GPUCoreTemp' is an integer attribute.
'GPUCoreTemp' is a read-only attribute.
'GPUCoreTemp' can use the following target types: X Screen, GPU.
</pre>
<p>The GPU temps of this board is 41 C.
</p>
<p>In order to get just the temperature for use in utils such as <code>rrdtool</code> or <code>conky</code>, among others:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-settings -q gpucoretemp -t</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">41</pre>
<h3><span class="mw-headline" id="nvidia-smi">nvidia-smi</span></h3>
<p>Use nvidia-smi which can read temps directly from the GPU without the need to use X at all. This is important for a small group of users who do not have X running on their boxes, perhaps because the box is headless running server apps. 
To display the GPU temperature in the shell, use nvidia-smi as follows:
</p>
<pre>$ nvidia-smi
</pre>
<p>This should output something similar to the following:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-smi</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">
Fri Jan  6 18:53:54 2012       
+------------------------------------------------------+                       
| NVIDIA-SMI 2.290.10   Driver Version: 290.10         |                       
|-------------------------------+----------------------+----------------------+
| Nb.  Name                     | Bus Id        Disp.  | Volatile ECC SB / DB |
| Fan   Temp   Power Usage /Cap | Memory Usage         | GPU Util. Compute M. |
|===============================+======================+======================|
| 0.  GeForce 8500 GT           | 0000:01:00.0  N/A    |       N/A        N/A |
|  30%   62 C  N/A   N/A /  N/A |  17%   42MB /  255MB |  N/A      Default    |
|-------------------------------+----------------------+----------------------|
| Compute processes:                                               GPU Memory |
|  GPU  PID     Process name                                       Usage      |
|=============================================================================|
|  0.           ERROR: Not Supported                                          |
+-----------------------------------------------------------------------------+
</pre>
<p>Only for temperature:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-smi -q -d TEMPERATURE</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">

====NVSMI LOG====

Timestamp                           : Sun Apr 12 08:49:10 2015
Driver Version                      : 346.59

Attached GPUs                       : 1
GPU 0000:01:00.0
    Temperature
        GPU Current Temp            : 52 C
        GPU Shutdown Temp           : N/A
        GPU Slowdown Temp           : N/A

</pre>
<p>In order to get just the temperature for use in utils such as rrdtool or conky, among others:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">52</pre>
<p>Reference: <a rel="nofollow" class="external free" href="http://www.question-defense.com/2010/03/22/gpu-linux-shell-temp-get-nvidia-gpu-temperatures-via-linux-cli">http://www.question-defense.com/2010/03/22/gpu-linux-shell-temp-get-nvidia-gpu-temperatures-via-linux-cli</a>.
</p>
<h3><span class="mw-headline" id="nvclock">nvclock</span></h3>
<p>Use <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/nvclock/">nvclock</a></span><sup><small>AUR</small></sup> which is available from the <a href="../../en/Arch_User_Repository.html" title="AUR" class="mw-redirect">AUR</a>.
</p>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDDDFF; border: thin solid #BBBBDD; overflow: hidden;">
<strong> Note: </strong><code>nvclock</code> cannot access thermal sensors on newer NVIDIA cards such as Geforce 200 series cards.</div>
<p>There can be significant differences between the temperatures reported by nvclock and nvidia-settings/nv-control. According to <a rel="nofollow" class="external text" href="http://sourceforge.net/projects/nvclock/forums/forum/67426/topic/1906899">this post</a> by the author (thunderbird) of nvclock, the nvclock values should be more accurate.
</p>
<h2><span class="mw-headline" id="Set_fan_speed_at_login">Set fan speed at login</span></h2>
<div class="noprint archwiki-template-message">
<p><a href="../../File:Tango-edit-clear.png" class="image"><img alt="Tango-edit-clear.png" src="../../File:Tango-edit-clear.png" width="48" height="48"></a><b>This article or section needs language, wiki syntax or style improvements.</b><a href="../../File:Tango-edit-clear.png" class="image"><img alt="Tango-edit-clear.png" src="../../File:Tango-edit-clear.png" width="48" height="48"></a></p>
<div>
<b>Reason:</b> Refer to <a href="#Enabling_overclocking">#Enabling overclocking</a> for description of <i>Coolbits</i>. (Discuss in <a rel="nofollow" class="external text" href="https://wiki.archlinux.org/index.php/Talk:NVIDIA/Tips_and_tricks">Talk:NVIDIA/Tips and tricks#</a>)</div>
</div>
<p>You can adjust the fan speed on your graphics card with <i>nvidia-settings'</i> console interface. First ensure that your Xorg configuration sets the Coolbits option to <code>4</code>, <code>5</code> or <code>12</code> for fermi and above in your <code>Device</code> section to enable fan control.
</p>
<pre>Option "Coolbits" "4"
</pre>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDDDFF; border: thin solid #BBBBDD; overflow: hidden;">
<strong> Note: </strong>GeForce 400/500 series cards cannot currently set fan speeds at login using this method. This method only allows for the setting of fan speeds within the current X session by way of nvidia-settings.</div>
<p>Place the following line in your <a href="../../en/Xinit.html" title="Xinitrc" class="mw-redirect">xinitrc</a> file to adjust the fan when you launch Xorg. Replace <code><i>n</i></code> with the fan speed percentage you want to set.
</p>
<pre>nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUCurrentFanSpeed=<i>n</i>"
</pre>
<p>You can also configure a second GPU by incrementing the GPU and fan number.
</p>
<pre>nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUCurrentFanSpeed=<i>n</i>" \
                -a "[gpu:1]/GPUFanControlState=1" -a  [fan:1]/GPUCurrentFanSpeed=<i>n</i>" &amp;
</pre>
<p>If you use a login manager such as GDM or KDM, you can create a desktop entry file to process this setting. Create <code>~/.config/autostart/nvidia-fan-speed.desktop</code> and place this text inside it. Again, change <code><i>n</i></code> to the speed percentage you want.
</p>
<pre>[Desktop Entry]
Type=Application
Exec=nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUCurrentFanSpeed=<i>n</i>"
X-GNOME-Autostart-enabled=true
Name=nvidia-fan-speed
</pre>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDDDFF; border: thin solid #BBBBDD; overflow: hidden;">
<strong> Note: </strong>Since the drivers version 349.16, <code>GPUCurrentFanSpeed</code> has to be replaced with <code>GPUTargetFanSpeed</code>.<a rel="nofollow" class="external autonumber" href="https://devtalk.nvidia.com/default/topic/821563/linux/can-t-control-fan-speed-with-beta-driver-349-12/post/4526208/#4526208">[1]</a>
</div>
<h2><span class="mw-headline" id="Manual_configuration">Manual configuration</span></h2>
<p>Several tweaks (which cannot be enabled <a href="../../en/NVIDIA.html#Automatic_configuration" title="NVIDIA">automatically</a> or with the <a href="../../en/NVIDIA.html#NVIDIA_Settings" title="NVIDIA">GUI</a>) can be performed by editing your <a href="../../en/NVIDIA.html#Minimal_configuration" title="NVIDIA">config</a> file. The Xorg server will need to be restarted before any changes are applied.
</p>
<p>See <a rel="nofollow" class="external text" href="ftp://download.nvidia.com/XFree86/Linux-x86/355.11/README/README.txt">NVIDIA Accelerated Linux Graphics Driver README and Installation Guide</a> for additional details and options.
</p>
<h3><span class="mw-headline" id="Disabling_the_logo_on_startup">Disabling the logo on startup</span></h3>
<p>Add the <code>"NoLogo"</code> option under section <code>Device</code>:
</p>
<pre>Option "NoLogo" "1"
</pre>
<h3><span class="mw-headline" id="Overriding_monitor_detection">Overriding monitor detection</span></h3>
<p>The <code>"ConnectedMonitor"</code> option under section <code>Device</code> allows to override monitor detection when X server starts, which may save a significant amount of time at start up. The available options are: <code>"CRT"</code> for analog connections, <code>"DFP"</code> for digital monitors and <code>"TV"</code> for televisions.
</p>
<p>The following statement forces the NVIDIA driver to bypass startup checks and recognize the monitor as DFP:
</p>
<pre>Option "ConnectedMonitor" "DFP"
</pre>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDDDFF; border: thin solid #BBBBDD; overflow: hidden;">
<strong> Note: </strong> Use "CRT" for all analog 15 pin VGA connections, even if the display is a flat panel. "DFP" is intended for DVI, HDMI, or DisplayPort digital connections only.</div>
<h3><span class="mw-headline" id="Enabling_brightness_control">Enabling brightness control</span></h3>
<p>Add under section <code>Device</code>:
</p>
<pre>Option "RegistryDwords" "EnableBrightnessControl=1"
</pre>
<p>If brightness control still does not work with this option, try installing <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/nvidia-bl/">nvidia-bl</a></span><sup><small>AUR</small></sup> or <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/nvidiabl/">nvidiabl</a></span><sup><small>AUR</small></sup>.
</p>
<h3><span class="mw-headline" id="Enabling_SLI">Enabling SLI</span></h3>
<div style="padding: 5px; margin: 0.50em 0; background-color: #FFDDDD; border: thin solid #DDBBBB; overflow: hidden;">
<strong> Warning: </strong>As of May 7, 2011, you may experience sluggish video performance in GNOME 3 after enabling SLI.</div>
<div style="padding: 5px; margin: 0.50em 0; background-color: #FFDDDD; border: thin solid #DDBBBB; overflow: hidden;">
<strong> Warning: </strong>Since the GTX 10xx Series (1080, 1070, 1060, etc) only 2-way SLI is supported. 3-way and 4-way SLI may work for CUDA/OpenCL applications, but will most likely break all OpenGL applications.</div>
<p>Taken from the NVIDIA driver's <a rel="nofollow" class="external text" href="ftp://download.nvidia.com/XFree86/Linux-x86/355.11/README/xconfigoptions.html">README</a> Appendix B: <i>This option controls the configuration of SLI rendering in supported configurations.</i> A "supported configuration" is a computer equipped with an SLI-Certified Motherboard and 2 or 3 SLI-Certified GeForce GPUs. See NVIDIA's <a rel="nofollow" class="external text" href="http://www.slizone.com/page/home.html">SLI Zone</a> for more information.
</p>
<p>Find the first GPU's PCI Bus ID using <code>lspci</code>:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ lspci | grep VGA</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">
03:00.0 VGA compatible controller: nVidia Corporation G92 [GeForce 8800 GTS 512] (rev a2)
05:00.0 VGA compatible controller: nVidia Corporation G92 [GeForce 8800 GTS 512] (rev a2)
</pre>
<p>Add the BusID (3 in the previous example) under section <code>Device</code>:
</p>
<pre>BusID "PCI:3:0:0"
</pre>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDDDFF; border: thin solid #BBBBDD; overflow: hidden;">
<strong> Note: </strong>The format is important. The BusID value must be specified as <code>"PCI:&lt;BusID&gt;:0:0"</code>
</div>
<p>Add the desired SLI rendering mode value under section <code>Screen</code>:
</p>
<pre>Option "SLI" "AA"
</pre>
<p>The following table presents the available rendering modes.
</p>
<table class="wikitable">
<tr>
<th> Value </th>
<th> Behavior
</th>
</tr>
<tr>
<td> 0, no, off, false, Single </td>
<td> Use only a single GPU when rendering.
</td>
</tr>
<tr>
<td> 1, yes, on, true, Auto </td>
<td> Enable SLI and allow the driver to automatically select the appropriate rendering mode.
</td>
</tr>
<tr>
<td> AFR </td>
<td> Enable SLI and use the alternate frame rendering mode.
</td>
</tr>
<tr>
<td> SFR </td>
<td> Enable SLI and use the split frame rendering mode.
</td>
</tr>
<tr>
<td> AA </td>
<td> Enable SLI and use SLI antialiasing. Use this in conjunction with full scene antialiasing to improve visual quality.
</td>
</tr>
</table>
<p>Alternatively, you can use the <code>nvidia-xconfig</code> utility to insert these changes into <code>xorg.conf</code> with a single command:
</p>
<pre># nvidia-xconfig --busid=PCI:3:0:0 --sli=AA
</pre>
<p>To verify that SLI mode is enabled from a shell:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-settings -q all | grep SLIMode</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">
  Attribute 'SLIMode' (arch:0.0): AA 
    'SLIMode' is a string attribute.
    'SLIMode' is a read-only attribute.
    'SLIMode' can use the following target types: X Screen.
</pre>
<div style="padding: 5px; margin: 0.50em 0; background-color: #FFDDDD; border: thin solid #DDBBBB; overflow: hidden;">
<strong> Warning: </strong> After enabling SLI, your system may become frozen/non-responsive upon starting xorg. It is advisable that you disable your display manager before restarting.</div>
<h3><span class="mw-headline" id="Enabling_overclocking">Enabling overclocking</span></h3>
<div style="padding: 5px; margin: 0.50em 0; background-color: #FFDDDD; border: thin solid #DDBBBB; overflow: hidden;">
<strong> Warning: </strong>Please note that overclocking may damage hardware and that no responsibility may be placed on the authors of this page due to any damage to any information technology equipment from operating products out of specifications set by the manufacturer.</div>
<p>Overclocking is controlled via <i>Coolbits</i> option in the <code>Device</code> section, which enables various unsupported features:
</p>
<pre>Option "Coolbits" "<i>value</i>"
</pre>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDFFDD; border: thin solid #BBDDBB; overflow: hidden;">
<strong> Tip: </strong>The <i>Coolbits</i> option can be easily controlled with the <i>nvidia-xconfig</i>, which manipulates the Xorg configuration files: <pre># nvidia-xconfig --cool-bits=<i>value</i></pre>
</div>
<p>The <i>Coolbits</i> value is the sum of its component bits in the binary numeral system. The component bits are:
</p>
<ul>
<li> <code>1</code> (bit 0) - Enables overclocking of older (pre-Fermi) cores on the <i>Clock Frequencies</i> page in <i>nvidia-settings</i>.</li>
<li> <code>2</code> (bit 1) - When this bit is set, the driver will "attempt to initialize SLI when using GPUs with different amounts of video memory".</li>
<li> <code>4</code> (bit 2) - Enables manual configuration of GPU fan speed on the <i>Thermal Monitor</i> page in <i>nvidia-settings</i>.</li>
<li> <code>8</code> (bit 3) - Enables overclocking on the <i>PowerMizer</i> page in <i>nvidia-settings</i>. Available since version 337.12 for the Fermi architecture and newer.<a rel="nofollow" class="external autonumber" href="http://www.phoronix.com/scan.php?px=MTY1OTM&amp;page=news_item">[2]</a>
</li>
<li> <code>16</code> (bit 4) - Enables overvoltage using <i>nvidia-settings</i> CLI options. Available since version 346.16 for the Fermi architecture and newer.<a rel="nofollow" class="external autonumber" href="http://www.phoronix.com/scan.php?page=news_item&amp;px=MTg0MDI">[3]</a>
</li>
</ul>
<p>To enable multiple features, add the <i>Coolbits</i> values together. For example, to enable overclocking and overvoltage of Fermi cores, set <code>Option "Coolbits" "24"</code>.
</p>
<p>The documentation of <i>Coolbits</i> can be found in <code>/usr/share/doc/nvidia/html/xconfigoptions.html</code>. Driver version 346.16 documentation on <i>Coolbits</i> can be found online <a rel="nofollow" class="external text" href="ftp://download.nvidia.com/XFree86/Linux-x86/355.11/README/xconfigoptions.html">here</a>.
</p>
<div style="padding: 5px; margin: 0.50em 0; background-color: #DDDDFF; border: thin solid #BBBBDD; overflow: hidden;">
<strong> Note: </strong>An alternative is to edit and reflash the GPU BIOS either under DOS (preferred), or within a Win32 environment by way of <a rel="nofollow" class="external text" href="http://www.mvktech.net/component/option,com_remository/Itemid,26/func,select/id,127/orderby,2/page,1/">nvflash</a><sup>[<a href="https://en.wikipedia.org/wiki/Wikipedia:Link_rot" class="extiw" title="wikipedia:Wikipedia:Link rot">dead link</a> 2013-05-25]</sup> and <a rel="nofollow" class="external text" href="http://www.mvktech.net/component/option,com_remository/Itemid,26/func,select/id,135/orderby,2/page,1/">NiBiTor 6.0</a><sup>[<a href="https://en.wikipedia.org/wiki/Wikipedia:Link_rot" class="extiw" title="wikipedia:Wikipedia:Link rot">dead link</a> 2013-05-25]</sup>. The advantage of BIOS flashing is that not only can voltage limits be raised, but stability is generally improved over software overclocking methods such as Coolbits. <a rel="nofollow" class="external text" href="http://ivanvojtko.blogspot.sk/2014/03/how-to-overclock-geforce-460gtx-fermi.html">Fermi BIOS modification tutorial</a>
</div>
<h4><span class="mw-headline" id="Setting_static_2D.2F3D_clocks">Setting static 2D/3D clocks</span></h4>
<p>Set the following string in the <code>Device</code> section to enable PowerMizer at its maximum performance level (VSync will not work without this line):
</p>
<pre>Option "RegistryDwords" "PerfLevelSrc=0x2222"
</pre>
</div>
<div id="catlinks" class="catlinks">
<div id="mw-normal-catlinks" class="mw-normal-catlinks">
<a href="../../Special:Categories.html" title="Special:Categories">Categories</a>: <ul>
<li><a href="../../en/Category:Graphics.html" title="Category:Graphics">Graphics</a></li>
<li><a href="../../en/Category:X_server.html" title="Category:X server">X server</a></li>
</ul>
</div>
<div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul>
<li><a href="../../en/Category:Pages_or_sections_flagged_with_Template:Style.html" title="Category:Pages or sections flagged with Template:Style">Pages or sections flagged with Template:Style</a></li>
<li><a href="../../en/Category:Pages_with_dead_links.html" title="Category:Pages with dead links">Pages with dead links</a></li>
</ul>
</div>
</div>					<div class="visualClear"></div>
				</div>
			</div>
		</div>
		<div class="visualClear"></div>
					<div id="footer" role="contentinfo">
						<ul id="f-list" style="margin: 0 2em">
									<li>
Retrieved from "<a dir="ltr" href="https://wiki.archlinux.org/index.php?title=NVIDIA/Tips_and_tricks&amp;oldid=446524">https://wiki.archlinux.org/index.php?title=NVIDIA/Tips_and_tricks&amp;oldid=446524</a>"</li>
					<li id="lastmod"> This page was last modified on 13 August 2016, at 14:59.</li>
									<li id="copyright">Content is available under <a class="external" rel="nofollow" href="http://www.gnu.org/copyleft/fdl.html">GNU Free Documentation License 1.3 or later</a> unless otherwise noted.</li>
									<br><li id="privacy"><a href="../../en/ArchWiki:General_disclaimer.html" title="ArchWiki:Privacy policy">Privacy policy</a></li>
									<li id="about"><a href="../../en/ArchWiki:About.html" title="ArchWiki:About">About ArchWiki</a></li>
									<li id="disclaimer"><a href="../../en/ArchWiki:General_disclaimer.html" title="ArchWiki:General disclaimer">Disclaimers</a></li>
							</ul>
		</div>
		</div>
		</body>
</html>
